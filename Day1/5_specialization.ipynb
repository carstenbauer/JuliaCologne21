{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While abstraction in the form of duck typing and generic programming is great for us as programmers the computer needs specific machine instructions operating on specific data structures. Hence, Julia needs to **specialize** generic code, that is compile specific native versions of the code for specific input data types. **The better the specialization the faster the code!** In the following we will investigate how Julia achieves good code specialization while retaining the power of generic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Julia fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia isn't fast *per se*.\n",
    "\n",
    "One can write terribly slow code in any language, including Julia.\n",
    "\n",
    "So let's ask a different question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Can* Julia be fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Microbenchmarks\n",
    " <img src=\"imgs/benchmarks.svg\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vandermonde matrix (once again)\n",
    "(modified from [Steven's Julia intro](https://web.mit.edu/18.06/www/Fall17/1806/julia/Julia-intro.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}V=\\begin{bmatrix}1&\\alpha _{1}&\\alpha _{1}^{2}&\\dots &\\alpha _{1}^{n-1}\\\\1&\\alpha _{2}&\\alpha _{2}^{2}&\\dots &\\alpha _{2}^{n-1}\\\\1&\\alpha _{3}&\\alpha _{3}^{2}&\\dots &\\alpha _{3}^{n-1}\\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\1&\\alpha _{m}&\\alpha _{m}^{2}&\\dots &\\alpha _{m}^{n-1}\\end{bmatrix}\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np = pyimport(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vander(1:5, increasing=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code for this function is [here](https://github.com/numpy/numpy/blob/v1.16.1/numpy/lib/twodim_base.py#L475-L563). It calls `np.multiply.accumulate` which is implemented in C [here](https://github.com/numpy/numpy/blob/deea4983aedfa96905bbaee64e3d1de84144303f/numpy/core/src/umath/ufunc_object.c#L3678). However, this code doesn't actually perform the computation, it basically only checks types and stuff. The actual kernel that gets called is [here](https://github.com/numpy/numpy/blob/deea4983aedfa96905bbaee64e3d1de84144303f/numpy/core/src/umath/loops.c.src#L1742). This isn't even C code but a template for C code which is used to generate type specific kernels.\n",
    "\n",
    "Overall, this setup only supports a limited set of types, like `Float64`, `Float32`, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple Julia implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vander(x::AbstractVector{T}) where T\n",
    "    m = length(x)\n",
    "    V = Matrix{T}(undef, m, m)\n",
    "    for j = 1:m\n",
    "        V[j,1] = one(x[j])\n",
    "    end\n",
    "    for i= 2:m\n",
    "        for j = 1:m\n",
    "            V[j,i] = x[j] * V[j,i-1]\n",
    "            end\n",
    "        end\n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vander(1:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick speed comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Show Code</summary>\n",
    "<br>\n",
    "    \n",
    "```julia\n",
    "using BenchmarkTools, Plots\n",
    "ns = exp10.(range(1, 4, length=30));\n",
    "\n",
    "tnp = Float64[]\n",
    "tjl = Float64[]\n",
    "for n in ns\n",
    "    x = 1:n |> collect\n",
    "    push!(tnp, @belapsed np.vander(\\$x) samples=3 evals=1)\n",
    "    push!(tjl, @belapsed vander(\\$x) samples=3 evals=1)\n",
    "end\n",
    "plot(ns, tnp./tjl, m=:circle, xscale=:log10, xlab=\"matrix size\", ylab=\"NumPy time / Julia time\", legend=:false)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"imgs/vandermonde.svg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the clean and concise Julia implementation is **beating numpy's C implementation for small matrices** and is **on-par for large matrix sizes**.\n",
    "\n",
    "At the same time, the Julia code is *generic* and works for arbitrary types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vander(Int32[4, 8, 16, 32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It even works for non-numerical types. The only requirement is that the type has a *one* (identity element) and a multiplication operation defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vander([\"this\", \"is\", \"a\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `one(String) == \"\"` since the empty string is the identity under multiplication (string concatenation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can Julia be fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imgs/from_source_to_native.png\" alt=\"drawing\" width=\"800\"/></p>\n",
    " \n",
    "**AST = Abstract Syntax Tree**\n",
    "\n",
    "**SSA = Static Single Assignment**\n",
    "\n",
    "**[LLVM](https://de.wikipedia.org/wiki/LLVM) = Low Level Virtual Machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization and code inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Julia specializes on the types of function arguments**, i.e. Julia compiles efficient machine code for the given input types, **when a function is called for the first time**.\n",
    "\n",
    "If it is called again, the already existing machine code is reused, until we call the function with different input types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(x,y) = 2x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.2, 3.4, 5.6]\n",
    "y = [0.4, 0.7, 0.9]\n",
    "\n",
    "@time func(x,y);\n",
    "@time func(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First call:** compilation + running the code\n",
    "\n",
    "**Second call:** running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time func(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one of the input types changes, Julia compiles a new specialization of the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time func(x,y);\n",
    "@time func(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two efficient native codes in the cache: one for all `Vector{Float64}` inputs and another one for `Vector{Int64}` as the first and `Vector{Float64}` as the second argument type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *But I really want to see what happens!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the code at all transformation stages with a bunch of macros:\n",
    "\n",
    "* The AST after parsing (**`@macroexpand`**)\n",
    "* The AST after lowering (**`@code_typed`**, **`@code_warntype`**)\n",
    "* The AST after type inference and optimization (**`@code_lowered`**)\n",
    "* The LLVM IR (**`@code_llvm`**)\n",
    "* The assembly machine code (**`@code_native`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed func(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_lowered func(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm func(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the comments (lines starting with `;` using `debuginfo=:none`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_llvm debuginfo=:none func(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none func(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to `Float64` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none func(1.2,2.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How important is code specialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to estimate the performance gain by specialization.\n",
    "\n",
    "We wrap our numbers into a custom type which internally stores them as `Any` to prevent specialization.\n",
    "\n",
    "(This is qualitatively comparable to what Python does.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Anything\n",
    "    value::Any\n",
    "end\n",
    "\n",
    "operation(x::Number) = x^2 + sqrt(x)\n",
    "operation(x::Anything) = x.value^2 + sqrt(x.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime operation(2.0);\n",
    "\n",
    "x = Anything(2.0)\n",
    "@btime operation($x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's about an 40 times slowdown!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none operation(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none operation(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make runtime the fun time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia specializes on input types (not values). As a rule of thumb: **only type information is available to the compiler when specializing code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scientific computations, we typically run a piece of code many times over and over again. Think of a Monte Carlo simulation, for example, where we perform the update and the Metropolis check millions of times.\n",
    "\n",
    "**Therefore, we want our runtime to be as short as possible.**\n",
    "\n",
    "On the other hand, for a given set of input arguments, Julia compiles the piece of code only once, as we have seen above. The time it takes to compile our code is almost always negligible compared to the duration of the full computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Determinant of a 2x2 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your task would be to write a function computing the determinant of a 2x2 matrix. How would you implement it?\n",
    "\n",
    "Probably you'd say, well I know the formula for computing the determinant of a 2x2 matrix! Let's just implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_2x2(X::AbstractMatrix) = X[1,1] * X[2,2] - X[1,2] * X[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [1 2; 3 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_2x2(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime det_2x2(M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how Julia's built-in `det` function compares to our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "det(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime det(M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much slower!! But why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that, as we've discussed above, the compiler only has the type information available for producing a specialization of the `det` function, i.e. in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in particular, the **size of the matrix is not encoded in the type and therefore not available to the compiler!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(typeof(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in spirit, when calling `det(M)` for the first time, we ask the compiler to create a specialization which can handle matrices of all sizes! That's obviously much harder than the 2x2 case. Hence, the produced code is more general - it will also work for 3x3 matrices etc. - but much less efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@code_typed debuginfo=:none det(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move the size information to the type domain and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StaticArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = SMatrix{2,2}(1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or equivalently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = @SMatrix [1 2; 3 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For static arrays, we can extract the matrix size solely from the type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(typeof(S)) # doesn't error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the compiler can utilize the fact that `S` is a 2x2 matrix - it can create different specializations for different matrix sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime det(S);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, let us check what `det(S)` is actually doing under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_typed debuginfo=:none det(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate this into something more readable:\n",
    "```julia\n",
    "%2 = %3 = S[1,1] = 1\n",
    "%4 = %5 = S[2,1] = 3\n",
    "%6 = %7 = S[1,2] = 2\n",
    "%8 = %9 = S[2,2] = 4\n",
    "\n",
    "%10 = %3 * %9 = S[1,1] * S[2,2]\n",
    "%11 = %7 * %5 = S[1,2] * S[2,1]\n",
    "%12 = %10 - %11 = S[1,1] * S[2,2] - S[1,2] * S[2,1]\n",
    "```\n",
    "\n",
    "**Overall it just corresponds to the explicit formula that we hand-coded in `det_2x2`!**\n",
    "```julia\n",
    "det(S) = S[1,1] * S[2,2] - S[1,2] * S[2,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_native debuginfo=:none det(Mstatic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Side remark: of course, here it is not really the compiler that does the job but it gets help from multiple dispatch: StaticArrays.jl just implements a specific method of the function `det` for the 2x2 case, see `@edit det(S)`. But you hopefully get the point.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@edit det(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other operations sped up by using StaticArrays.jl**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "============================================\n",
    "    Benchmarks for 3Ã—3 Float64 matrices\n",
    "============================================\n",
    "Matrix multiplication               -> 5.9x speedup\n",
    "Matrix multiplication (mutating)    -> 1.8x speedup\n",
    "Matrix addition                     -> 33.1x speedup\n",
    "Matrix addition (mutating)          -> 2.5x speedup\n",
    "Matrix determinant                  -> 112.9x speedup\n",
    "Matrix inverse                      -> 67.8x speedup\n",
    "Matrix symmetric eigendecomposition -> 25.0x speedup\n",
    "Matrix Cholesky decomposition       -> 8.8x speedup\n",
    "Matrix LU decomposition             -> 6.1x speedup\n",
    "Matrix QR decomposition             -> 65.0x speedup\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, by putting more information in the type you are putting more stress on the compiler to optimize things. If the static arrays are too big compile time might explode or the compiler might just give up and fall back to a slow default version.\n",
    "\n",
    "Hence, static arrays are only useful as small fixed-size arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # might take longer to compile and the speedup is gone\n",
    "# N = 20\n",
    "# M = rand(N,N);\n",
    "# m = SMatrix{N,N}(M);\n",
    "\n",
    "# println(\"Inversion\")\n",
    "# @btime inv($m);\n",
    "# @btime inv($M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are explicit type annotations necessary? (like in C or Fortran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Julia's type inference is powerful. Specifying types **is not** necessary for best performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_function(x)\n",
    "    y = rand()\n",
    "    z = rand()\n",
    "    x+y+z\n",
    "end\n",
    "\n",
    "function my_function_typed(x::Int)::Float64\n",
    "    y::Float64 = rand()\n",
    "    z::Float64 = rand()\n",
    "    x+y+z\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime my_function(10);\n",
    "@btime my_function_typed(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " However, annotating types explicitly can serve a purpose.\n",
    "\n",
    "* **Define a user interface/type filter** (will throw error if incompatible type is given)\n",
    "* Enforce conversions\n",
    "* Rarely, help the compiler infer types in tricky situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core messages of this Notebook\n",
    "\n",
    "* Julia **can be fast.**\n",
    "* **A function is compiled when called for the first time** with a given set of argument types.\n",
    "* The are **multiple compilation steps** all of which can be inspected through macros like `@code_warntype`.\n",
    "* **Code specialization** based on the types of all of the input arguments is important for speed.\n",
    "* Calculations can be moved to compile-time to make run-time faster.\n",
    "* In virtually all cases, **explicit type annotations are irrelevant for performance**.\n",
    "* Type annotations in function signatures define a **type filter/user interface**."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
